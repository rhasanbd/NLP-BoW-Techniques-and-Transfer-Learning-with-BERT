{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for Text Analytics\n",
    "\n",
    "In this notebook we will learn how to perform **feature extraction** for text analytics.\n",
    "\n",
    "More specifically, we will go through various steps of feature extraction. The knowledge gained from this notebook will be used to implement Machine Learning algorithms for:\n",
    "- Text Clustering (e.g., Topic Modeling)\n",
    "- Text Classification (e.g., Naive Bayes classifier)\n",
    "- Word Embedding (e.g., Word2Vector)\n",
    "- Document Embedding (e.g., Doc2Vector)\n",
    "\n",
    "\n",
    "## What is feature extraction?\n",
    "\n",
    "Text Analysis is a major application field for Machine Learning algorithms. However the raw data (a sequence of symbols) cannot be fed directly to the algorithms. Because the ML algorithms expect **numerical feature vectors** with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "Thus, it is important to know how to extract numerical feature vectors from the text data for performing machine based text analysis.\n",
    "\n",
    "        Feature extraction consists in transforming arbitrary data, such as text or images, into numerical features usable for Machine Learning.\n",
    "\n",
    "\n",
    "## What is a feature?\n",
    "\n",
    "Each individual token (e.g., words in a text) occurrence frequency (normalized or not) is treated as a feature.\n",
    "\n",
    "The vector of all the token frequencies for a given document is considered a multivariate sample.\n",
    "\n",
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "We call <strong><font color=red size=4>vectorization</font></strong> the general process of turning a collection of text documents into numerical feature vectors. This vectorization strategy is known as the <font color=blue>Bag of Words or “Bag of n-grams”</font> representation. Documents are described by word occurrences while **completely ignoring the relative position information of the words** in the document.\n",
    "\n",
    "\n",
    "## Steps in Feature Extraction\n",
    "\n",
    "Generally there are three steps in feature extraction.\n",
    "1. Text Normalization (Stemming & Lemmatization)\n",
    "2. Text Preprocessing (Tokenization, removing stop words, etc.)\n",
    "3. Vectorization of the features\n",
    "\n",
    "\n",
    "\n",
    "## Python Libraries for Feature Extraction\n",
    "\n",
    "We will use python libraries such as the **Natural Language Tool Kit (NLTK)** and Scikit-Learn to extract numerical features from text contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> 1. Text Normalization by Stemming & Lemmatization </font>\n",
    "\n",
    "Before we do text preprocessing (e.g., tokenize, remove stop words, etc.) and convert to vectors of numbers, sometimes it is useful to normalize the text.\n",
    "\n",
    "\n",
    "\n",
    "## What is Text Normalization?\n",
    "\n",
    "Languages we speak and write are made up of several words often **derived from one another**. When a language contains words that are derived from another word as their use in the speech changes is called Inflected Language.\n",
    "\n",
    "Text normalization reduces words to their root form, as shown in the examples below.\n",
    "\n",
    "- The boy's cars are different colors --> the boy car be differ color\n",
    "- Playing, Plays, Played -> Play (common root)\n",
    "- am, are, is --> be (common root)\n",
    "- Car, cars, car's, cars' --> car\n",
    "\n",
    "\n",
    "In **Natural Language Processing** there are two commonly used Text Normalization (or sometimes called Word Normalization) techniques:\n",
    "- Stemming\n",
    "- Lemmatization \n",
    "\n",
    "Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960's.\n",
    "\n",
    "\n",
    "#### But stemming and Lemmatization do normalization in different ways!\n",
    "\n",
    "\n",
    "\n",
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their **root forms** such as mapping a group of words to the same stem even if the stem itself is <strong><font color=red>not a valid word</font></strong> in the Language. \n",
    "\n",
    "For example, books —> book, looked —> look. \n",
    "\n",
    "There are two stemming algorithms:\n",
    "- Porter stemming algorithm (removes common morphological and inflexional endings from words)\n",
    "- Lancaster stemming algorithm (a more aggressive stemming algorithm) \n",
    "\n",
    "PorterStemmer is the oldest one originally developed in 1979. LancasterStemmer was developed in 1990 and uses a more aggressive approach than Porter Stemming Algorithm.\n",
    "\n",
    "### Lemmatization\n",
    " \n",
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the <strong><font color=red>root word belongs to the language</font></strong>. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases (e.g., dictionary) to get the correct base forms of words.\n",
    "\n",
    "In Lemmatization, a root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, **dictionary form**, or citation form of a set of words.\n",
    "\n",
    "\n",
    "## Stemming & Lemmatization using Python\n",
    "\n",
    "Python provides the Natural Language Tool Kit (NLTK) library to make programs that work with natural language. It has a user-friendly interface to datasets that are over 50 corpora and lexical resources such as <strong><font color=blue size=4>WordNet</font></strong> word repository. The library can perform different operations such as tokenizing, stemming, classification, parsing, tagging, and semantic reasoning.\n",
    "\n",
    "\n",
    "### Installing NLTK:\n",
    "To install nltk use the pip installer:\n",
    "- pip install nltk\n",
    "\n",
    "\n",
    "## Stemming vs. Lemmatization\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, **lemma is an actual language word**.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, we use WordNet corpus and a corpus for stop words as well to produce lemma which makes it **slower than stemming**. We also have to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "    So when to use what? \n",
    "\n",
    "The above points show that if speed is focused then stemming should be used since lemmatizers scan a corpus which consumes time and processing. It depends on the application we are working on that decides if stemmers should be used or lemmatizers. If we are building a language application in which language is important we should use lemmatization as it uses a corpus to match root forms.\n",
    "\n",
    "\n",
    "For more detail see the following URL:\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/hasan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hasan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download Wordnet through NLTK\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing Using the WordNetLemmatizer\n",
    "\n",
    "While doing lemmatization, it is useful to remember:\n",
    "- Lemmatization works **only on individual words**. Thus, we need to tokenize a document first.\n",
    "- Unlike stemming, lemmatization **does not work on capitalized words**. Thus, we need to convert a word into lowercase before performning lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some lemmatized words:\n",
      "bats -> bat\n",
      "are -> are\n",
      "feet -> foot\n",
      "plays -> play\n",
      "plays -> blasphemious\n",
      "BLASHEPHEMERS -> BLASHEPHEMERS\n",
      "blashephemers -> blashephemers\n",
      "believing -> believing\n",
      "\n",
      "Example Sentence:  The students received grades from the Professor's webpage.\n",
      "\n",
      "Tokenized Words:\n",
      "['The', 'students', 'received', 'grades', 'from', 'the', 'Professor', \"'s\", 'webpage', '.']\n",
      "\n",
      "Lemmatized Output:\n",
      "The student received grade from the Professor 's webpage .\n"
     ]
    }
   ],
   "source": [
    "# Create the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize Single Words\n",
    "\n",
    "print(\"Some lemmatized words:\")\n",
    "print(\"bats -> %s\" % lemmatizer.lemmatize(\"bats\"))\n",
    "\n",
    "print(\"are -> %s\" % lemmatizer.lemmatize(\"are\"))\n",
    "\n",
    "print(\"feet -> %s\" % lemmatizer.lemmatize(\"feet\"))\n",
    "\n",
    "print(\"plays -> %s\" % lemmatizer.lemmatize(\"plays\"))\n",
    "\n",
    "print(\"plays -> %s\" % lemmatizer.lemmatize(\"blasphemious\"))\n",
    "\n",
    "print(\"BLASHEPHEMERS -> %s\" % lemmatizer.lemmatize(\"BLASHEPHEMERS\"))\n",
    "\n",
    "print(\"blashephemers -> %s\" % lemmatizer.lemmatize(\"blashephemers\"))\n",
    "\n",
    "print(\"believing -> %s\" % lemmatizer.lemmatize(\"believing\"))\n",
    "\n",
    "\n",
    "\n",
    "# Lemmatization works only on individual words. Thus, we need to tokenize a document/sentence first.\n",
    "\n",
    "# Define a sentence to be lemmatized\n",
    "sentence = \"The students received grades from the Professor's webpage.\"\n",
    "print(\"\\nExample Sentence: \", sentence)\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(\"\\nTokenized Words:\")\n",
    "print(word_list)\n",
    "\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(\"\\nLemmatized Output:\")\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Using the PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stemmed words:\n",
      "bats -> bat\n",
      "are -> are\n",
      "feet -> feet\n",
      "plays -> play\n",
      "believing -> believ\n",
      "blashephemers -> blashephem\n",
      "BLASHEPHEMERS -> blashephem\n",
      "\n",
      "Tokenized Words:\n",
      "['The', 'students', 'received', 'grades', 'from', 'the', 'Professor', \"'s\", 'webpage', '.']\n",
      "\n",
      "Stemmed Output (Porter):\n",
      "the student receiv grade from the professor 's webpag .\n"
     ]
    }
   ],
   "source": [
    "# Create the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "print(\"Some stemmed words:\")\n",
    "print(\"bats -> %s\" % stemmer.stem(\"bats\"))\n",
    "\n",
    "print(\"are -> %s\" % stemmer.stem(\"are\"))\n",
    "\n",
    "print(\"feet -> %s\" % stemmer.stem(\"feet\"))\n",
    "\n",
    "print(\"plays -> %s\" % stemmer.stem(\"plays\"))\n",
    "\n",
    "\n",
    "print(\"believing -> %s\" % stemmer.stem(\"believing\"))\n",
    "\n",
    "print(\"blashephemers -> %s\" % stemmer.stem(\"blashephemers\"))\n",
    "\n",
    "print(\"BLASHEPHEMERS -> %s\" % stemmer.stem(\"BLASHEPHEMERS\"))\n",
    "\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(\"\\nTokenized Words:\")\n",
    "print(word_list)\n",
    "\n",
    "    \n",
    "stemmed_output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "print(\"\\nStemmed Output (Porter):\")\n",
    "print(stemmed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Using the LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stemmed words:\n",
      "bats -> bat\n",
      "are -> ar\n",
      "feet -> feet\n",
      "plays -> play\n",
      "\n",
      "Stemmed Output (Lancaster):\n",
      "the stud receiv grad from the profess 's webp .\n"
     ]
    }
   ],
   "source": [
    "# Create the Lancaster stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(\"Some stemmed words:\")\n",
    "print(\"bats -> %s\" % stemmer.stem(\"bats\"))\n",
    "\n",
    "print(\"are -> %s\" % stemmer.stem(\"are\"))\n",
    "\n",
    "print(\"feet -> %s\" % stemmer.stem(\"feet\"))\n",
    "\n",
    "print(\"plays -> %s\" % stemmer.stem(\"plays\"))\n",
    "\n",
    "stemmed_output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "print(\"\\nStemmed Output (Lancaster):\")\n",
    "print(stemmed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=maroon> Observation about Stemming & Lemmatization </font>\n",
    "\n",
    "We draw two useful observations about stemming and lemmatization in the context of text classification.\n",
    "\n",
    "- Lemmatization is a **more suitable technique** for word normalization in text classification. Because, unlike stemming, it reduces inflected words to the lemma (canonical) words that exists in the language.\n",
    "- If stemming should be used, then **Porter stemmer is preferable**. Lancaster stemmer is more aggressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> 2. Text Preprocessing (tokenization, removing stop words, etc.) & 3. Vectorization of the features</font>\n",
    "\n",
    "\n",
    "In order to perform Machine Learning on text documents, first we need to turn the text contents into numerical feature vectors.\n",
    "\n",
    "\n",
    "## Bag of words\n",
    "\n",
    "The most intuitive way to do so is to use a \"bag of words\" representation:\n",
    "\n",
    "        Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices). \n",
    "        \n",
    "        For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary.\n",
    "\n",
    "The bag of words representation implies that n_features is the number of distinct words in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing & Feature Vectorization\n",
    "\n",
    "The Scikit-Learn CountVectorizer() object implements **both text preprocessing and feature vectorization** in a single class.\n",
    "\n",
    "It converts a collection of text documents to a matrix of token counts. It produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "For example, it creates a set of $d$ unique words (referred as tokens) from the collection of documents. Then, each document is represented by a d-dimensional feature vector. Each component of this vector represents the occurance count of the feature (term or word) in that document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to lemmatize a sentence\n",
    "def sentenceLemmatizer(sentence):\n",
    "    # Tokenize: Split the sentence into words\n",
    "    word_list = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Lemmatize list of words and join\n",
    "    lemmatized_sentence = ' '.join([lemmatizer.lemmatize(w.lower()) for w in word_list])\n",
    "    \n",
    "    return lemmatized_sentence\n",
    "    \n",
    "    \n",
    "\n",
    "# # Define a sentence to be lemmatized\n",
    "# sentence = \"The students received grades from the Professor's WEBPAGES.\"\n",
    "# print(\"\\nExample Sentence: \", sentence)\n",
    "\n",
    "\n",
    "# lemmatized_output = sentenceLemmatizer(sentence)\n",
    "# print(\"\\nLemmatized Output:\")\n",
    "# print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documents:\n",
      "Document 1: This book is good.\n",
      "Document 2: Good books are good to read.\n",
      "\n",
      "Lemmatized Documents:\n",
      "Document 1: this book is good .\n",
      "Document 2: good book are good to read .\n",
      "\n",
      "Feature Names:\n",
      "['are', 'book', 'good', 'is', 'read', 'this', 'to']\n",
      "\n",
      "Vocabulary:  {'this': 5, 'book': 1, 'is': 3, 'good': 2, 'are': 0, 'to': 6, 'read': 4}\n",
      "Note: After each word the index of that word is given. It's not word count.\n",
      "\n",
      "Get the index of the words from the vocabulary:\n",
      "Vocabulary - Index of good:  2\n",
      "Vocabulary - Index of awesome:  None\n",
      "\n",
      "Count Vector Matrix (Dense Matrix):\n",
      "[[0 1 1 1 0 1 0]\n",
      " [1 1 2 0 1 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = (\n",
    "\"This book is good.\",\n",
    "\"Good books are good to read.\"\n",
    ")\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "# Display the original documents\n",
    "print(\"Original Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Lemmatize the documents using the \"sentenceLemmatizer\" function defined above\n",
    "j = 1\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = sentenceLemmatizer(documents[i])\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Display the lemmatized documents\n",
    "print(\"\\nLemmatized Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Create a count vectorizer objet\n",
    "count_vect = CountVectorizer(lowercase=True)\n",
    "\n",
    "\n",
    "# Create a matrix representation of the documents\n",
    "# Each row represents a single document\n",
    "# Each column represents the term frequecy for each feature\n",
    "document_counts = count_vect.fit_transform(documents).todense()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(count_vect.get_feature_names())\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary: \", count_vect.vocabulary_)\n",
    "print(\"Note: After each word the index of that word is given. It's not word count.\")\n",
    "\n",
    "print(\"\\nGet the index of the words from the vocabulary:\")\n",
    "print(\"Vocabulary - Index of good: \", count_vect.vocabulary_.get(\"good\"))\n",
    "print(\"Vocabulary - Index of awesome: \", count_vect.vocabulary_.get(\"awesome\"))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nCount Vector Matrix (Dense Matrix):\")\n",
    "#print(document_counts.toarray())\n",
    "print(document_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Preprocessing: Removing Stop Words\n",
    "\n",
    "Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text.\n",
    "\n",
    "The stop words may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.\n",
    "\n",
    "    To remove the stop words, set the \"stop_words\" attribute value of the CountVectorizer to 'english'.\n",
    "\n",
    "Note that if the value is set to ‘english’, a built-in stop word list for English is used. \n",
    "\n",
    "However, there are several known issues with ‘english’:\n",
    "\n",
    "URL: http://aclweb.org/anthology/W18-2502\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documents:\n",
      "Document 1: This book is good.\n",
      "Document 2: Good books are good to read.\n",
      "\n",
      "Lemmatized Documents:\n",
      "Document 1: this book is good .\n",
      "Document 2: good book are good to read .\n",
      "\n",
      "Feature Names:\n",
      "['book', 'good', 'read']\n",
      "\n",
      "Vocabulary:  {'book': 0, 'good': 1, 'read': 2}\n",
      "\n",
      "Count Vector Matrix (Dense Matrix):\n",
      "[[1 1 0]\n",
      " [1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(lowercase=True, stop_words='english')\n",
    "document_counts = count_vect.fit_transform(documents)\n",
    "\n",
    "documents = (\n",
    "\"This book is good.\",\n",
    "\"Good books are good to read.\"\n",
    ")\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "# Display the original documents\n",
    "print(\"Original Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Lemmatize the documents\n",
    "j = 1\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = sentenceLemmatizer(documents[i])\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Display the lemmatized documents\n",
    "print(\"\\nLemmatized Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(count_vect.get_feature_names())\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "\n",
    "print(\"\\nCount Vector Matrix (Dense Matrix):\")\n",
    "print(document_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vectorization: TF-IDF Term Weighting\n",
    "\n",
    "In a large text corpus, some words will be very present (e.g., “the”, “a”, “is” in English), hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "\n",
    "In order to **re-weight the count features into floating point** values suitable for usage by a classifier it is very common to use the **term frequency–inverse document frequency (tf–idf)** transform.\n",
    "\n",
    "There are two ways to implement the tf-idf transform:\n",
    "\n",
    "- First compute the occurance counting and then apply tf-idf transformer (CountVectorizer and TfidfTransformer)\n",
    "- Use TfidfVectorizer that combines CountVectorizer and TfidfTransformer in a single model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documents:\n",
      "Document 1: This book is good.\n",
      "Document 2: Good books are good to read.\n",
      "\n",
      "Lemmatized Documents:\n",
      "Document 1: this book is good .\n",
      "Document 2: good book are good to read .\n",
      "\n",
      "Feature Names:\n",
      "['are', 'book', 'good', 'is', 'read', 'this', 'to']\n",
      "\n",
      "Vocabulary:\n",
      "{'this': 5, 'book': 1, 'is': 3, 'good': 2, 'are': 0, 'to': 6, 'read': 4}\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.40993715 0.40993715 0.57615236 0.         0.57615236\n",
      "  0.        ]\n",
      " [0.42519636 0.30253071 0.60506143 0.         0.42519636 0.\n",
      "  0.42519636]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents).todense()\n",
    "\n",
    "documents = (\n",
    "\"This book is good.\",\n",
    "\"Good books are good to read.\"\n",
    ")\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "# Display the original documents\n",
    "print(\"Original Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Lemmatize the documents\n",
    "j = 1\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = sentenceLemmatizer(documents[i])\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Display the lemmatized documents\n",
    "print(\"\\nLemmatized Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary:\")\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "#print(tfidf_matrix.toarray())\n",
    "print(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vectorization: Counting Binary Occurances\n",
    "\n",
    "In some scenarios we are interested about the binary occurrence markers for the features. \n",
    "\n",
    "For example, very short texts are likely to have noisy tf–idf values while the binary occurrence info is more stable.\n",
    "\n",
    "Also, some estimators such as <strong><font color=red size=4>Multivariate Bernoulli</font></strong> Naive Bayes explicitly models discrete boolean random variables. \n",
    "\n",
    "We can count the binary occurances of the features by using the \"binary\" attribute of CountVectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: this book is good .\n",
      "Document 2: good book are good to read .\n",
      "\n",
      "Feature Names:\n",
      "['book', 'good', 'read']\n",
      "\n",
      "Vocabulary:  {'book': 0, 'good': 1, 'read': 2}\n",
      "\n",
      "Size of Vocabulary:  3\n",
      "\n",
      "Count Vector Matrix\n",
      "[[1 1 0]\n",
      " [1 1 1]]\n",
      "\n",
      "Dimension of Count Vector Matrix:  (2, 3)\n",
      "\n",
      "Row 1 of the Matrix:  [1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# binary : boolean, default=False\n",
    "# If True, all non zero counts are set to 1. \n",
    "# This is useful for discrete probabilistic models that model binary events rather than integer counts.\n",
    "\n",
    "count_vect = CountVectorizer(lowercase=True, binary=True, stop_words='english')\n",
    "document_counts = count_vect.fit_transform(documents)\n",
    "\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "\n",
    "# Lemmatize the documents\n",
    "j = 1\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = sentenceLemmatizer(documents[i])\n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "# Display the documents\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(count_vect.get_feature_names())\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "print(\"\\nSize of Vocabulary: \", len(count_vect.vocabulary_))\n",
    "\n",
    "\n",
    "print(\"\\nCount Vector Matrix\")\n",
    "print(document_counts.toarray())\n",
    "\n",
    "\n",
    "print(\"\\nDimension of Count Vector Matrix: \", document_counts.toarray().shape)\n",
    "\n",
    "\n",
    "print(\"\\nRow 1 of the Matrix: \", document_counts.toarray()[0, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of the Bag of Words representation\n",
    "\n",
    "\n",
    "The bag of words model is a collection of unigrams. There are some limitations of this model:\n",
    "- It cannot capture phrases and multi-word expressions.\n",
    "- It effectively disregards any word order dependence. \n",
    "- It doesn’t account for potential misspellings or word derivations.\n",
    "\n",
    "A better and sophisticated model for feature representation is the n-grams model. Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.\n",
    "\n",
    "The n-grams model provides a set of co-occuring words within a given window. When computing the n-grams we typically move $n$ word forward. For example, consider the sentence \"The woods are lovely, dark and deep\". If n = 2 (bigrams), then the ngrams would be:\n",
    "\n",
    "- the woods\n",
    "- woods are\n",
    "- are lovely\n",
    "- lovely dark\n",
    "- dark and\n",
    "- and deep\n",
    "\n",
    "\n",
    "We might alternatively consider a collection of character n-grams, a representation resilient against misspellings and derivations.\n",
    "\n",
    "## How to Use the n-grams Model:\n",
    "To use the n-grams model we need to set the following two attributes of the CountVectorizer.\n",
    "\n",
    "- ngram_range : tuple (min_n, max_n)\n",
    "\n",
    "        The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "\n",
    "- analyzer : string, {‘word’, ‘char’, ‘char_wb’} or callable\n",
    "\n",
    "        Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: this book is good .\n",
      "Document 2: good book are good to read .\n",
      "\n",
      "Feature Names:\n",
      "['are', 'are good', 'book', 'book are', 'book is', 'good', 'good book', 'good to', 'is', 'is good', 'read', 'this', 'this book', 'to', 'to read']\n",
      "\n",
      "Vocabulary:  {'this': 11, 'book': 2, 'is': 8, 'good': 5, 'this book': 12, 'book is': 4, 'is good': 9, 'are': 0, 'to': 13, 'read': 10, 'good book': 6, 'book are': 3, 'are good': 1, 'good to': 7, 'to read': 14}\n",
      "\n",
      "Count Vector Matrix\n",
      "[[0 0 1 0 1 1 0 0 1 1 0 1 1 0 0]\n",
      " [1 1 1 1 0 2 1 1 0 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# To create bigrams, set the \"ngram_range\" to the tuple (1, 2)\n",
    "count_vect = CountVectorizer(lowercase=True, analyzer=\"word\", ngram_range=(1, 2))\n",
    "document_counts = count_vect.fit_transform(documents)\n",
    "\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(count_vect.get_feature_names())\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "\n",
    "print(\"\\nCount Vector Matrix\")\n",
    "print(document_counts.toarray())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
