{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for NLP: Using the Bag of Words Model\n",
    "\n",
    "In this notebook, we will learn how to perform **feature extraction** for Natural Language Processing (NLP). Each token (e.g., word or sub-word) in a text is treated as a feature.\n",
    "\n",
    "Feature extraction is a critical step in NLP, involving the transformation of raw data (e.g., text) into numerical features suitable for machine learning (ML). This process is also known as feature or token embedding.\n",
    "\n",
    "## Steps in Feature Extraction\n",
    "\n",
    "Typically, feature extraction involves four key steps:\n",
    "\n",
    "1. **Text Standardization** (stemming & lemmatization)\n",
    "2. **Text Preprocessing** (removing stop words & tokenization)\n",
    "3. **Vocabulary Construction and Indexing**\n",
    "4. **Vectorization of the Features**\n",
    "\n",
    "There are two primary types of feature vectorization models:\n",
    "\n",
    "- **Word order agnostic**: the Bag of Words (BoW) model\n",
    "- **Word order preserving**: sequence models\n",
    "\n",
    "In this notebook, we will specifically use the BoW model for feature vectorization.\n",
    "\n",
    "\n",
    "## Python Libraries for the BoW-based Feature Extraction\n",
    "\n",
    "We will use Python libraries such as the **Natural Language Tool Kit (NLTK)** and Scikit-Learn to extract numerical features from text contents.nts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> 1. Text Standardization by Stemming & Lemmatization </font>\n",
    "\n",
    "Before we do text preprocessing (e.g., tokenize, remove stop words, etc.) and convert to vectors of numbers, sometimes it is useful to standardize the text.\n",
    "\n",
    "\n",
    "## What is Text Standardization?\n",
    "\n",
    "Languages we speak and write are made up of many words, often **derived from one another**. When a language contains words that are modified based on their use in speech, it is called an inflected language.\n",
    "\n",
    "Text standardization reduces words to their root form, as shown in the examples below.\n",
    "\n",
    "- The boy's cars are different colors --> the boy car be differ color\n",
    "- Playing, Plays, Played -> Play (common root)\n",
    "- am, are, is --> be (common root)\n",
    "- Car, cars, car's, cars' --> car\n",
    "\n",
    "\n",
    "In **NLP** there are two commonly used text standardization techniques:\n",
    "- Stemming\n",
    "- Lemmatization \n",
    "\n",
    "Stemming and Lemmatization have been studied, and algorithms have been developed in Computer Science since the 1960s.\n",
    "\n",
    "\n",
    "#### But stemming and Lemmatization do standardization in different ways!\n",
    "\n",
    "\n",
    "\n",
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their **root forms** such as mapping a group of words to the same stem even if the stem itself is <strong><font color=red>not a valid word</font></strong> in the Language. \n",
    "\n",
    "For example, books —> book, looked —> look. \n",
    "\n",
    "There are two stemming algorithms:\n",
    "- Porter stemming algorithm (removes common morphological and inflexional endings from words)\n",
    "- Lancaster stemming algorithm (a more aggressive stemming algorithm) \n",
    "\n",
    "PorterStemmer is the oldest one originally developed in 1979. LancasterStemmer was developed in 1990 and uses a more aggressive approach than the Porter Stemming Algorithm.\n",
    "\n",
    "### Lemmatization\n",
    " \n",
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the <strong><font color=red>root word belongs to the language</font></strong>. As opposed to stemming, lemmatization does not simply chop off inflections. Instead, it uses lexical knowledge bases (e.g., dictionary) to get the correct base forms of words.\n",
    "\n",
    "In lemmatization, a root word is called a lemma. A lemma (plural lemmas or lemmata) is the canonical form, **dictionary form**, or citation form of a set of words.\n",
    "\n",
    "\n",
    "## Stemming & Lemmatization using Python\n",
    "\n",
    "Python provides the Natural Language Tool Kit (NLTK) library to make programs that work with natural language. It has a user-friendly interface to datasets that are over 50 corpora and lexical resources such as <strong><font color=blue size=4>WordNet</font></strong> word repository. The library can perform different operations such as tokenizing, stemming, classification, parsing, tagging, and semantic reasoning.\n",
    "\n",
    "\n",
    "### Installing NLTK:\n",
    "To install nltk use the pip installer:\n",
    "- pip install nltk\n",
    "\n",
    "\n",
    "## Stemming vs. Lemmatization\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, **lemma is an actual language word**.\n",
    "\n",
    "Stemming follows a set algorithm that applies steps directly to words, making it faster. In contrast, lemmatization relies on the WordNet corpus and a stop word corpus to produce the lemma, which makes it **slower than stemming**. Additionally, lemmatization requires defining parts of speech to generate the correct lemma.\n",
    "\n",
    "    So, when should each method be used?\n",
    "\n",
    "The choice depends on the application's needs. If speed is a priority, stemming is preferable since lemmatizers must scan a corpus, which requires more time and processing. However, if language accuracy is essential—such as in applications where correct word forms matter—lemmatization is more suitable because it matches words to their root forms through a linguistic corpus.\n",
    "\n",
    "\n",
    "For more details see the following URL:\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/mhasan2/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/mhasan2/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mhasan2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mhasan2/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Download Wordnet through NLTK\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing Using the WordNetLemmatizer\n",
    "\n",
    "While doing lemmatization, it is useful to remember:\n",
    "- Lemmatization works **only on individual words**. Thus, we need to tokenize a document first.\n",
    "- Unlike stemming, lemmatization **does not work on capitalized words**. Thus, we need to convert a word into lowercase before performning lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some lemmatized words:\n",
      "bats -> bat\n",
      "are -> are\n",
      "feet -> foot\n",
      "plays -> play\n",
      "blasphemious -> blasphemious\n",
      "BLASHEPHEMERS -> BLASHEPHEMERS\n",
      "blashephemers -> blashephemers\n",
      "believing -> believing\n",
      "\n",
      "Example Sentence:  The students received grades from the Professor's webpage.\n",
      "\n",
      "Tokenized Sentence:\n",
      "['The', 'students', 'received', 'grades', 'from', 'the', 'Professor', \"'s\", 'webpage', '.']\n",
      "\n",
      "Lemmatized Tokens:\n",
      "The student received grade from the Professor 's webpage .\n"
     ]
    }
   ],
   "source": [
    "# Create the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize Single Words\n",
    "\n",
    "print(\"Some lemmatized words:\")\n",
    "print(\"bats -> %s\" % lemmatizer.lemmatize(\"bats\"))\n",
    "\n",
    "print(\"are -> %s\" % lemmatizer.lemmatize(\"are\"))\n",
    "\n",
    "print(\"feet -> %s\" % lemmatizer.lemmatize(\"feet\"))\n",
    "\n",
    "print(\"plays -> %s\" % lemmatizer.lemmatize(\"plays\"))\n",
    "\n",
    "print(\"blasphemious -> %s\" % lemmatizer.lemmatize(\"blasphemious\"))\n",
    "\n",
    "print(\"BLASHEPHEMERS -> %s\" % lemmatizer.lemmatize(\"BLASHEPHEMERS\"))\n",
    "\n",
    "print(\"blashephemers -> %s\" % lemmatizer.lemmatize(\"blashephemers\"))\n",
    "\n",
    "print(\"believing -> %s\" % lemmatizer.lemmatize(\"believing\"))\n",
    "\n",
    "\n",
    "\n",
    "# Lemmatization works only on individual words. Thus, we need to tokenize a document/sentence first.\n",
    "\n",
    "# Define a sentence to be lemmatized\n",
    "sentence = \"The students received grades from the Professor's webpage.\"\n",
    "print(\"\\nExample Sentence: \", sentence)\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(\"\\nTokenized Sentence:\")\n",
    "print(word_list)\n",
    "\n",
    "\n",
    "# Lemmatize the list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(\"\\nLemmatized Tokens:\")\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Using the PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stemmed words:\n",
      "bats -> bat\n",
      "are -> are\n",
      "feet -> feet\n",
      "plays -> play\n",
      "blasphemious -> blasphemious\n",
      "BLASHEPHEMERS -> blashephem\n",
      "blashephemers -> blashephem\n",
      "believing -> believ\n",
      "\n",
      "Tokenized Sentence:\n",
      "['The', 'students', 'received', 'grades', 'from', 'the', 'Professor', \"'s\", 'webpage', '.']\n",
      "\n",
      "Stemmed Tokens (Porter):\n",
      "the student receiv grade from the professor 's webpag .\n"
     ]
    }
   ],
   "source": [
    "# Create the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "print(\"Some stemmed words:\")\n",
    "print(\"bats -> %s\" % stemmer.stem(\"bats\"))\n",
    "\n",
    "print(\"are -> %s\" % stemmer.stem(\"are\"))\n",
    "\n",
    "print(\"feet -> %s\" % stemmer.stem(\"feet\"))\n",
    "\n",
    "print(\"plays -> %s\" % stemmer.stem(\"plays\"))\n",
    "\n",
    "print(\"blasphemious -> %s\" % lemmatizer.lemmatize(\"blasphemious\"))\n",
    "\n",
    "print(\"BLASHEPHEMERS -> %s\" % stemmer.stem(\"BLASHEPHEMERS\"))\n",
    "\n",
    "print(\"blashephemers -> %s\" % stemmer.stem(\"blashephemers\"))\n",
    "\n",
    "print(\"believing -> %s\" % stemmer.stem(\"believing\"))\n",
    "\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(\"\\nTokenized Sentence:\")\n",
    "print(word_list)\n",
    "\n",
    "    \n",
    "stemmed_output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "print(\"\\nStemmed Tokens (Porter):\")\n",
    "print(stemmed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Using the LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some stemmed words:\n",
      "bats -> bat\n",
      "are -> ar\n",
      "feet -> feet\n",
      "plays -> play\n",
      "\n",
      "Stemmed Tokens (Lancaster):\n",
      "the stud receiv grad from the profess 's webp .\n"
     ]
    }
   ],
   "source": [
    "# Create the Lancaster stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(\"Some stemmed words:\")\n",
    "print(\"bats -> %s\" % stemmer.stem(\"bats\"))\n",
    "\n",
    "print(\"are -> %s\" % stemmer.stem(\"are\"))\n",
    "\n",
    "print(\"feet -> %s\" % stemmer.stem(\"feet\"))\n",
    "\n",
    "print(\"plays -> %s\" % stemmer.stem(\"plays\"))\n",
    "\n",
    "stemmed_output = ' '.join([stemmer.stem(w) for w in word_list])\n",
    "print(\"\\nStemmed Tokens (Lancaster):\")\n",
    "print(stemmed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=maroon> Observation about Stemming & Lemmatization </font>\n",
    "\n",
    "We can make two important observations about stemming and lemmatization in the context of text classification:\n",
    "\n",
    "- Lemmatization is a **more suitable technique** for token standardization in text classification. Unlike stemming, which may produce non-existent or truncated forms of words, lemmatization reduces inflected words to their canonical forms (lemmas) that are recognized in the language.\n",
    "\n",
    "- If stemming is to be used, the **Porter stemmer is the preferred choice**. The Lancaster stemmer is more aggressive in its approach and may lead to over-stemming, which can negatively impact the quality of the text classification.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> 2. Text Preprocessing (removing stop words & tokenization), 3. Vocabulary Construction and Indexing, and 4. Vectorization of the features using the BoW Model</font>\n",
    "\n",
    "\n",
    "## Bag of Words Model\n",
    "\n",
    "In a Bag of Words (BoW) model, documents are represented by the occurrences of words or tokens. This model completely disregards the relative positions of the words or tokens within the document. It is referred to as a \"bag\" of words because all information regarding the order or structure of words is discarded. The focus of the BoW model is solely on whether known words appear in the document, rather than their specific locations. \n",
    "\n",
    "The underlying intuition is that documents with similar content are likely to be similar overall. Furthermore, the content alone can provide insights into the type of document. \n",
    "\n",
    "The BoW model can vary in complexity, which arises from two key considerations: \n",
    "- Designing the vocabulary of known words (or tokens)\n",
    "- Scoring the presence of these known words\n",
    "\n",
    "In the BoW model, tokenization typically employs the n-gram method, where tokens are formed by grouping n consecutive words. Tokens can represent single words (unigrams), pairs of words (bigrams), sequences of three words (trigrams), or even individual characters. \n",
    "\n",
    "Within the BoW framework, there are two primary methods for vectorizing features:\n",
    "- **Count Vector**: This represents the binary count of tokens or the frequency of token occurrences.\n",
    "- **TF-IDF Vector**: This method adjusts the word frequency by considering the importance of words across the entire document collection.\n",
    "\n",
    "First, we will present the count vectorization BoW technique.\n",
    "\n",
    "\n",
    "## Count Vectorization BoW Technique\n",
    "\n",
    "We will explore two techniques for count vectorization:\n",
    "\n",
    "- **Counting the frequency of tokens**\n",
    "- **Using binary counts of tokens**\n",
    "\n",
    "### Steps for Count Vectorization\n",
    "\n",
    "1. **Word Assignment**: Assign a fixed integer ID to each word that appears in any document within the training set. This is achieved by constructing a dictionary that maps each word to its corresponding integer index.\n",
    "\n",
    "2. **Counting Occurrences or Binary Count**: For each document \\( i \\), either count the number of occurrences of each word \\( w \\) or assign a binary count (1 if the word is present, 0 if it is not). Store this value in \\( X[i, j] \\) for feature \\( j \\), where \\( j \\) is the index of word \\( w \\) in the dictionary.\n",
    "\n",
    "The Bag of Words representation indicates that the number of features \\( n\\_features \\) corresponds to the total number of distinct words in the corpus.\n",
    "pus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing and BoW Feature Vectorization: Counting the Frequency of Tokens\n",
    "\n",
    "The Scikit-Learn CountVectorizer() object implements **both text preprocessing and BoW feature vectorization** in a single class.\n",
    "\n",
    "It converts a collection of text documents to a matrix of token counts. It produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "For example, it creates a set of $d$ unique words (referred to as tokens) from the collection of documents. Then, each document is represented by a d-dimensional feature vector. Each component of this vector represents the occurrence count of the feature (term or word) in that document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to lemmatize a sentence\n",
    "def sentenceLemmatizer(sentence):\n",
    "    # Tokenize: Split the sentence into words\n",
    "    word_list = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Lemmatize the list of words and join\n",
    "    lemmatized_sentence = ' '.join([lemmatizer.lemmatize(w.lower()) for w in word_list])\n",
    "    \n",
    "    return lemmatized_sentence\n",
    "    \n",
    "    \n",
    "\n",
    "# # Define a sentence to be lemmatized\n",
    "# sentence = \"The students received grades from the Professor's WEBPAGES.\"\n",
    "# print(\"\\nExample Sentence: \", sentence)\n",
    "\n",
    "\n",
    "# lemmatized_output = sentenceLemmatizer(sentence)\n",
    "# print(\"\\nLemmatized Output:\")\n",
    "# print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documents:\n",
      "Document 1: This book is good.\n",
      "Document 2: Good books are good to read.\n",
      "\n",
      "Lemmatized Documents:\n",
      "Document 1: this book is good .\n",
      "Document 2: good book are good to read .\n",
      "\n",
      "Feature Names:\n",
      "['are' 'book' 'good' 'is' 'read' 'this' 'to']\n",
      "\n",
      "Vocabulary:  {'this': 5, 'book': 1, 'is': 3, 'good': 2, 'are': 0, 'to': 6, 'read': 4}\n",
      "Note: After each word the index of that word is given. It's not word count.\n",
      "\n",
      "Get the index of the words from the vocabulary:\n",
      "Vocabulary - Index of good:  2\n",
      "Vocabulary - Index of awesome:  None\n",
      "\n",
      "Count Vector Matrix (Dense Matrix):\n",
      "[[0 1 1 1 0 1 0]\n",
      " [1 1 2 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a set of documents\n",
    "documents = (\n",
    "\"This book is good.\",\n",
    "\"Good books are good to read.\"\n",
    ")\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "# Display the original documents\n",
    "print(\"Original Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Lemmatize the documents using the \"sentenceLemmatizer\" function defined above\n",
    "j = 1\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = sentenceLemmatizer(documents[i])\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Display the lemmatized documents\n",
    "print(\"\\nLemmatized Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# BoW feature vectorization: Create a count vectorizer object\n",
    "count_vect = CountVectorizer(lowercase=True)\n",
    "\n",
    "\n",
    "# Create a matrix representation of the documents\n",
    "# Each row represents a single document\n",
    "# Each column represents the term frequency for each feature\n",
    "document_counts = count_vect.fit_transform(documents).todense()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(count_vect.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary: \", count_vect.vocabulary_)\n",
    "print(\"Note: After each word the index of that word is given. It's not word count.\")\n",
    "\n",
    "print(\"\\nGet the index of the words from the vocabulary:\")\n",
    "print(\"Vocabulary - Index of good: \", count_vect.vocabulary_.get(\"good\"))\n",
    "print(\"Vocabulary - Index of awesome: \", count_vect.vocabulary_.get(\"awesome\"))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nCount Vector Matrix (Dense Matrix):\")\n",
    "#print(document_counts.toarray())\n",
    "print(document_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Preprocessing: Removing Stop Words\n",
    "\n",
    "Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text.\n",
    "\n",
    "The stop words may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.\n",
    "\n",
    "    To remove the stop words, set the \"stop_words\" attribute value of the CountVectorizer to 'english'.\n",
    "\n",
    "Note that if the value is set to ‘english’, a built-in stop word list for English is used. \n",
    "\n",
    "However, there are several known issues with ‘english’:\n",
    "\n",
    "URL: http://aclweb.org/anthology/W18-2502\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documents:\n",
      "Document 1: This book is good.\n",
      "Document 2: Good books are good to read.\n",
      "\n",
      "Lemmatized Documents:\n",
      "Document 1: this book is good .\n",
      "Document 2: good book are good to read .\n",
      "\n",
      "Feature Names:\n",
      "['book' 'good' 'read']\n",
      "\n",
      "Vocabulary:  {'book': 0, 'good': 1, 'read': 2}\n",
      "\n",
      "Count Vector Matrix (Dense Matrix):\n",
      "[[1 1 0]\n",
      " [1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(lowercase=True, stop_words='english')\n",
    "document_counts = count_vect.fit_transform(documents)\n",
    "\n",
    "# Create a set of documents\n",
    "documents = (\n",
    "\"This book is good.\",\n",
    "\"Good books are good to read.\"\n",
    ")\n",
    "\n",
    "# Create an array of documents\n",
    "documents = np.array(documents)\n",
    "\n",
    "# Display the original documents\n",
    "print(\"Original Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Lemmatize the documents\n",
    "j = 1\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = sentenceLemmatizer(documents[i])\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Display the lemmatized documents\n",
    "print(\"\\nLemmatized Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(count_vect.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "\n",
    "print(\"\\nCount Vector Matrix (Dense Matrix):\")\n",
    "print(document_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Feature Vectorization: Using Binary Counts of Tokens\n",
    "\n",
    "In certain scenarios, we are particularly interested in binary occurrence markers for features.\n",
    "\n",
    "For instance, very short texts may yield noisy term frequency–inverse document frequency (tf-idf) values, while binary occurrence information tends to be more stable.\n",
    "\n",
    "Additionally, some estimators, such as **Multivariate Bernoulli Naive Bayes**, explicitly model discrete Boolean random variables.\n",
    "\n",
    "To count the binary occurrences of features, we can utilize the \"binary\" attribute of the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: this book is good .\n",
      "Document 2: good book are good to read .\n",
      "\n",
      "Feature Names:\n",
      "['book' 'good' 'read']\n",
      "\n",
      "Vocabulary:  {'book': 0, 'good': 1, 'read': 2}\n",
      "\n",
      "Size of Vocabulary:  3\n",
      "\n",
      "Count Vector Matrix\n",
      "[[1 1 0]\n",
      " [1 1 1]]\n",
      "\n",
      "Dimension of Count Vector Matrix:  (2, 3)\n",
      "\n",
      "Row 1 of the Matrix:  [1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# binary : boolean, default=False\n",
    "# If True, all non zero counts are set to 1. \n",
    "# This is useful for discrete probabilistic models that model binary events rather than integer counts.\n",
    "\n",
    "count_vect = CountVectorizer(lowercase=True, binary=True, stop_words='english')\n",
    "document_counts = count_vect.fit_transform(documents)\n",
    "\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "\n",
    "# Lemmatize the documents\n",
    "j = 1\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = sentenceLemmatizer(documents[i])\n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "# Display the documents\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(count_vect.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "print(\"\\nSize of Vocabulary: \", len(count_vect.vocabulary_))\n",
    "\n",
    "\n",
    "print(\"\\nCount Vector Matrix\")\n",
    "print(document_counts.toarray())\n",
    "\n",
    "\n",
    "print(\"\\nDimension of Count Vector Matrix: \", document_counts.toarray().shape)\n",
    "\n",
    "\n",
    "print(\"\\nRow 1 of the Matrix: \", document_counts.toarray()[0, :])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW Feature Vectorization: The TF-IDF Technique\n",
    "\n",
    "To this point, we have utilized the count vectorization method (which involves binary counts of tokens or frequency counts) for BoW vectorization. However, this approach has its limitations.\n",
    "\n",
    "In a large text corpus, certain words (such as “the,” “a,” and “is” in English) tend to appear very frequently, contributing minimal meaningful information about the actual content of the documents. If we were to feed this raw count data directly into a classifier, these common terms would overshadow the frequencies of rarer but more informative terms.\n",
    "\n",
    "To address this issue, we can **re-weight the count features into floating-point values** that are more suitable for use by a classifier. This is typically achieved using the **term frequency–inverse document frequency (tf-idf)** transformation.\n",
    "\n",
    "There are two main ways to implement the tf-idf transformation:\n",
    "\n",
    "1. First, compute the occurrence counts and then apply the tf-idf transformer using separate components (CountVectorizer followed by TfidfTransformer).\n",
    "2. Alternatively, use the TfidfVectorizer, which integrates CountVectorizer and TfidfTransformer into a single model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documents:\n",
      "Document 1: This book is good.\n",
      "Document 2: Good books are good to read.\n",
      "\n",
      "Lemmatized Documents:\n",
      "Document 1: this book is good .\n",
      "Document 2: good book are good to read .\n",
      "\n",
      "Feature Names:\n",
      "['are' 'book' 'good' 'is' 'read' 'this' 'to']\n",
      "\n",
      "Vocabulary:\n",
      "{'this': 5, 'book': 1, 'is': 3, 'good': 2, 'are': 0, 'to': 6, 'read': 4}\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.40993715 0.40993715 0.57615236 0.         0.57615236\n",
      "  0.        ]\n",
      " [0.42519636 0.30253071 0.60506143 0.         0.42519636 0.\n",
      "  0.42519636]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents).todense()\n",
    "\n",
    "# Create a set of documents\n",
    "documents = (\n",
    "\"This book is good.\",\n",
    "\"Good books are good to read.\"\n",
    ")\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "# Display the original documents\n",
    "print(\"Original Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Lemmatize the documents\n",
    "j = 1\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = sentenceLemmatizer(documents[i])\n",
    "    j += 1\n",
    "\n",
    "\n",
    "# Display the lemmatized documents\n",
    "print(\"\\nLemmatized Documents:\")\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary:\")\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "#print(tfidf_matrix.toarray())\n",
    "print(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of the Bag of Words Representation\n",
    "\n",
    "The BoW model considered so far is a collection of unigrams. However, it has several limitations:\n",
    "\n",
    "- It cannot capture phrases or multi-word expressions.\n",
    "- It effectively disregards any dependency on word order.\n",
    "- It does not account for potential misspellings or word derivations.\n",
    "\n",
    "A more sophisticated approach for feature representation is the n-grams model. Instead of constructing a simple collection of unigrams (n=1), one might opt for a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.\n",
    "\n",
    "The n-grams model provides a set of co-occurring words within a given context. When computing n-grams, we typically move \\( n \\) words forward. For example, consider the sentence \"The woods are lovely, dark and deep.\" If \\( n = 2 \\) (bigrams), the resulting n-grams would be:\n",
    "\n",
    "- the woods\n",
    "- woods are\n",
    "- are lovely\n",
    "- lovely dark\n",
    "- dark and\n",
    "- and deep\n",
    "\n",
    "Alternatively, we might consider a collection of character n-grams, which offers resilience against misspellings and word derivations.\n",
    "\n",
    "## How to Use the n-grams Model\n",
    "\n",
    "To implement the n-grams model, we need to set the following two attributes in the CountVectorizer:\n",
    "\n",
    "- **ngram_range**: tuple (min_n, max_n)  \n",
    "  Specifies the lower and upper boundaries of the range of n-values for different n-grams to be extracted. All values of \\( n \\) such that \\( \\text{min\\_n} \\leq n \\leq \\text{max\\_n} \\) will be used.\n",
    "\n",
    "- **analyzer**: string, {‘word’, ‘char’, ‘char_wb’} or callable  \n",
    "  Indicates whether the features should consist of word or character n-grams. The option ‘char_wb’ creates character n-grams only from text within word boundaries; n-grams at the edges of words are padded with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: this book is good .\n",
      "Document 2: good book are good to read .\n",
      "\n",
      "Feature Names:\n",
      "['are' 'are good' 'book' 'book are' 'book is' 'good' 'good book' 'good to'\n",
      " 'is' 'is good' 'read' 'this' 'this book' 'to' 'to read']\n",
      "\n",
      "Vocabulary:  {'this': 11, 'book': 2, 'is': 8, 'good': 5, 'this book': 12, 'book is': 4, 'is good': 9, 'are': 0, 'to': 13, 'read': 10, 'good book': 6, 'book are': 3, 'are good': 1, 'good to': 7, 'to read': 14}\n",
      "\n",
      "Count Vector Matrix\n",
      "[[0 0 1 0 1 1 0 0 1 1 0 1 1 0 0]\n",
      " [1 1 1 1 0 2 1 1 0 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# To create bigrams, set the \"ngram_range\" to the tuple (1, 2)\n",
    "count_vect = CountVectorizer(lowercase=True, analyzer=\"word\", ngram_range=(1, 2))\n",
    "document_counts = count_vect.fit_transform(documents)\n",
    "\n",
    "j = 1\n",
    "for i in documents:\n",
    "    print(\"Document %d: %s\" % (j, i))\n",
    "    j += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFeature Names:\")\n",
    "print(count_vect.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(\"\\nVocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "\n",
    "print(\"\\nCount Vector Matrix\")\n",
    "print(document_counts.toarray())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
